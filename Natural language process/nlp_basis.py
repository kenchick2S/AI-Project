# -*- coding: utf-8 -*-
"""NLP_basis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hTgVec1j0NTde3Is9RL8YQsiC7mXPNAs
"""

!pip install transformers

!pip install --upgrade --force-reinstall --no-deps kaggle

!kaggle -v

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
  
# Then move kaggle.json into the folder where the API expects to find it.
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c nlp-getting-started

!unzip nlp-getting-started.zip

import pandas as pd

sample = pd.read_csv('sample_submission.csv')

sample.head()

train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')

train_data.head()

train_data['text'][2]

set(train_data['target'])

test_data.head()

test_data['text'][3]

test_data.shape

import torch
from transformers import AutoTokenizer, AutoModel
from keras.preprocessing.sequence import pad_sequences

# coding: utf-8

# Tokenizer and Bert Model
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
embedding = AutoModel.from_pretrained('bert-base-uncased')


# Preprocess
sent = train_data['text'][0]
sent_token = tokenizer.encode(sent)
sent_token_padding = pad_sequences([sent_token], maxlen=30, padding='post', dtype='int')
masks = [[float(value>0) for value in values] for values in sent_token_padding]

print('sent:', sent)
print('sent_token:', sent_token)
print('sent_token_padding:', sent_token_padding)
print('masks:', masks)
print('\n')


# Convert
inputs = torch.tensor(sent_token_padding)
masks = torch.tensor(masks)
embedded = embedding(inputs, attention_mask=masks)
#print('embedded shape:', embedded.shape)

train_data.shape[0]

len(train_data['text'])

train_data['text'][0].split()

max = 0
for sent in train_data['text']:
  sent_token = tokenizer.encode(sent)
  if len(sent_token) > max:
    max = len(sent_token)
for sent in test_data['text']:
  sent_token = tokenizer.encode(sent)
  if len(sent_token) > max:
    max = len(sent_token)

max



decode

import torch
import torch.nn as nn
from torch.autograd import Variable
import torch.nn.functional as F
import time
import os
import re
from itertools import chain
from transformers import BertTokenizer
PRETRAINED_MODEL_NAME = "bert-base-uncased" #英文pretrain(不區分大小寫)
print(torch.__version__)

# get pre-train tokenizer
tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)
vocab = tokenizer.vocab
print("dict size", len(vocab))

# see some token and index mapping
import random
random_tokens = random.sample(list(vocab), 10)
random_ids = [vocab[t] for t in random_tokens]

print("{0:20}{1:15}".format("token", "index"))
print("-" * 25)
for t, id in zip(random_tokens, random_ids): #隨便看幾個字
    print("{0:15}{1:10}".format(t, id))

from sklearn.model_selection import train_test_split

train1, test1 = train_test_split(train_data, test_size=0.1)

train1.head()

train1.to_csv('train1.csv')

test1.to_csv('test1.csv')

train1_data = pd.read_csv('train1.csv')

train1_data.head()

TAG_RE = re.compile(r'<[^>]+>')
def preprocess_text(sen):
    # Removing html tags
    sentence = TAG_RE.sub('', sen)
    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', sentence)
    # Single character removal
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)
    # Removing multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)

    return sentence


def read_data(path, seg):
    data = []
    
    df = pd.read_csv(os.path.join(path, f"{seg}.csv"))
    for i in range(df.shape[0]):
      if seg=='train':
        data.append([preprocess_text(df['text'][i]), df['target'][i]])
      else:
        data.append(preprocess_text(df['text'][i]))
    return data

tttt = read_data('./', 'train')

len(tttt)

u = [tttt[i][1] for i in range(len(tttt))]
set(u)

from torch.utils.data import Dataset,random_split

TAG_RE = re.compile(r'[^\w\s]')
def preprocess_text(sen):
    # Removing html tags
    sentence = TAG_RE.sub('', sen)
    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', sentence)
    # Single character removal
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)
    # Removing multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)

    return sentence


def read_data(seg, path='/content/'):
    data = []
    
    df = pd.read_csv(os.path.join(path, f"{seg}.csv"))
    for i in range(df.shape[0]):
      if seg=='train':
        data.append([preprocess_text(df['text'][i]), df['target'][i]])
      else:
        data.append(preprocess_text(df['text'][i]))
    return data

#create Dataset
class MyDataset(Dataset):
    def __init__(self, mode, tokenizer):
        assert mode in ["train", "test"]  
        self.mode = mode
        self.df = read_data(mode) #its list [['text1',label],['text2',label],...]
        self.len = len(self.df)
        self.maxlen = 85 #限制文章長度(若你記憶體夠多也可以不限)
        self.tokenizer = tokenizer  # we will use BERT tokenizer
    
    # 定義回傳一筆訓練 / 測試數據的函式
    def __getitem__(self, idx):
        origin_text = self.df[idx][0]
        if self.mode == "test":
            origin_text = self.df[idx]
            text_a = self.df[idx]
            text_b = None  #for natural language inference
            #label_tensor = None #in our case, we have label
            label_id = None
            #label_tensor = torch.tensor(label_id)
        else:     
            text_a = self.df[idx][0]
            #print(self.df[idx][0])
            text_b = None  #for natural language inference
            label_id = self.df[idx][1]
            #print(self.df[idx][1])
            label_tensor = torch.tensor(label_id)
            
        
        # 建立第一個句子的 BERT tokens
        word_pieces = ["[CLS]"]
        tokens_a = self.tokenizer.tokenize(text_a)
        word_pieces += tokens_a[:self.maxlen] + ["[SEP]"]
        len_a = len(word_pieces)
        
        if text_b is not None:
            tokens_b = self.tokenizer.tokenize(text_b)
            word_pieces += tokens_b + ["[SEP]"]
            len_b = len(word_pieces) - len_a
               
        # 將整個 token 序列轉換成索引序列
        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)
        tokens_tensor = torch.tensor(ids)
        
        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句
        if text_b is None:
            segments_tensor = torch.tensor([1] * len_a,dtype=torch.long)
        elif text_b is not None:
            segments_tensor = torch.tensor([0] * len_a + [1] * len_b,dtype=torch.long)
        if self.mode == "test":
          return (tokens_tensor, segments_tensor, origin_text)
        else:
          return (tokens_tensor, segments_tensor, label_tensor, origin_text)
    
    def __len__(self):
        return self.len
    
# initialize Dataset
trainset = MyDataset("train", tokenizer=tokenizer)
testset = MyDataset("test", tokenizer=tokenizer)


#split val from trainset
val_size = int(trainset.__len__()*0.05) #比對LSTM 切出1000筆當validation
trainset, valset = random_split(trainset,[trainset.__len__()-val_size,val_size])
print('trainset size:' ,trainset.__len__())
print('valset size:',valset.__len__())
print('testset size: ',testset.__len__())

# 隨便選一個樣本
sample_idx = 10

# 利用剛剛建立的 Dataset 取出轉換後的 id tensors
tokens_tensor, segments_tensor, label_tensor, origin_text = trainset[sample_idx]

# 將 tokens_tensor 還原成文本
tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())

print('token:\n',tokens,'\n')
print('origin_text:\n',origin_text,'\n')
print('label:',int(label_tensor.numpy()),'\n')
print('tokens_tensor:\n',tokens_tensor,'\n')
print('segment tensor:\n',segments_tensor)
print(tokenizer.convert_tokens_to_ids(tokens))

# 隨便選一個樣本
sample_idx = 1

# 利用剛剛建立的 Dataset 取出轉換後的 id tensors
tokens_tensor, segments_tensor, origin_text = testset[sample_idx]

print('origin_text:\n',origin_text,'\n')

p = [1, 2]

p[0][1]

from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence

""""
create_mini_batch(samples)吃上面定義的mydataset
回傳訓練 BERT 時會需要的 4 個 tensors：
- tokens_tensors  : (batch_size, max_seq_len_in_batch)
- segments_tensors: (batch_size, max_seq_len_in_batch)
- masks_tensors   : (batch_size, max_seq_len_in_batch)
- label_ids       : (batch_size)
"""
#collate_fn: 如何將多個樣本的資料連成一個batch丟進 model
#截長補短後要限制attention只注意非pad 的部分
def create_mini_batch(samples):
    tokens_tensors = [s[0] for s in samples]
    segments_tensors = [s[1] for s in samples]
    
    # 訓練集有 labels
    try:
      if samples[0][2] is not None:
          label_ids = torch.stack([s[2] for s in samples])
      else:
          label_ids = None
    except:
      label_ids = None
    
    # zero pad到該batch下最長的長度
    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)
    segments_tensors = pad_sequence(segments_tensors,batch_first=True)
    
    # attention masks，將 tokens_tensors 裡頭不為 zero padding
    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens
    masks_tensors = torch.zeros(tokens_tensors.shape,dtype=torch.long)
    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)
    
    if label_ids == None:
      return tokens_tensors, segments_tensors, masks_tensors
    else:
      return tokens_tensors, segments_tensors, masks_tensors, label_ids


# 初始化一個每次回傳 batch size 個訓練樣本的 DataLoader
# 利用 'collate_fn' 將 list of samples 合併成一個 mini-batch
BATCH_SIZE = 16
trainloader = DataLoader(trainset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=True)
valloader = DataLoader(valset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=False)
testloader = DataLoader(testset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=False)

data = next(iter(trainloader))
tokens_tensors, segments_tensors, masks_tensors, label_ids = data
print(tokens_tensors)
print(segments_tensors)
print(masks_tensors)
print(label_ids)

from transformers import BertForSequenceClassification

NUM_LABELS = 2

model = BertForSequenceClassification.from_pretrained(
    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)


print("""
name      module
--------------------""")

for name, module in model.named_children():
    if name == "bert":
        for n, _ in module.named_children():
            print("{:10}{}".format(name,n) )
    else:
        print("{:10} {}".format(name, module))

from tqdm import tqdm
from sklearn.metrics import accuracy_score

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("device:",device)
model = model.to(device)


optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
EPOCHS = 5

for epoch in range(EPOCHS):
    correct = 0
    #total = 0
    train_loss , val_loss = 0.0 , 0.0
    train_acc, val_acc = 0, 0
    n, m = 0, 0
    model.train()
    for data in tqdm(trainloader):
        n += 1
        tokens_tensors, segments_tensors,masks_tensors, labels = [t.to(device) for t in data]

        # 將參數梯度歸零
        optimizer.zero_grad()
        
        # forward pass
        outputs = model(input_ids=tokens_tensors, 
                        token_type_ids=segments_tensors, 
                        attention_mask=masks_tensors, 
                        labels=labels)
        # outputs 的順序是 "(loss), logits, (hidden_states), (attentions)"
        
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        
        #get prediction and calulate acc
        logits = outputs[1]
        _, pred = torch.max(logits.data, 1)
        train_acc += accuracy_score(pred.cpu().tolist() , labels.cpu().tolist())

        # 紀錄當前 batch loss
        train_loss += loss.item()
    
    #validation
    with torch.no_grad():
        model.eval()
        for data in valloader:
            m += 1
            tokens_tensors, segments_tensors,masks_tensors, labels = [t.to(device) for t in data]
            val_outputs = model(input_ids=tokens_tensors, 
                        token_type_ids=segments_tensors, 
                        attention_mask=masks_tensors, 
                        labels=labels)
            
            logits = val_outputs[1]
            _, pred = torch.max(logits.data, 1)
            val_acc += accuracy_score(pred.cpu().tolist() , labels.cpu().tolist())
            val_loss += val_outputs[0].item()

    print('[epoch %d] loss: %.4f, acc: %.4f, val loss: %4f, val acc: %4f' %
          (epoch+1, train_loss/n, train_acc/n, val_loss/m,  val_acc/m  ))

print('Done')

test_data = pd.read_csv('test.csv')

result = {'id':[i for i in test_data['id']], 'target':[]}
with torch.no_grad():
  model.eval()
  for data in testloader:
      m += 1
      tokens_tensors, segments_tensors, masks_tensors = [t.to(device) for t in data]
      val_outputs = model(input_ids=tokens_tensors, 
                  token_type_ids=segments_tensors, 
                  attention_mask=masks_tensors)
      #print(val_outputs)
      logits = val_outputs[0]
      _, pred = torch.max(logits.data, 1)
      result['target'].append(pred.cpu().tolist())

result['target'] = [i for item in result['target'] for i in item]

result['target']

answer = pd.DataFrame(result)

answer.head()

answer.columns

len(answer['id'])

sample = pd.read_csv('sample_submission.csv')

sample.columns

sample.head()

answer.to_csv('answer.csv', index=False)

